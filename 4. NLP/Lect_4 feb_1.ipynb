{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdf271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibek Shiwakoti\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaeac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array (\n",
    "                        [\n",
    "                            'I love Brazil . Brazil!',\n",
    "                            'Sweden is best',\n",
    "                            'Germany beats both'\n",
    "                        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988873e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\travelmate\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff3851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count= CountVectorizer()\n",
    "bag_of_words=count.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eec7b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f030297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d8a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3f22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1: 'beats' - Value: 0\n",
      "Feature 2: 'best' - Value: 0\n",
      "Feature 3: 'both' - Value: 0\n",
      "Feature 4: 'brazil' - Value: 2\n",
      "Feature 5: 'germany' - Value: 0\n",
      "Feature 6: 'is' - Value: 0\n",
      "Feature 7: 'love' - Value: 1\n",
      "Feature 8: 'sweden' - Value: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the feature names (words) corresponding to each feature in the bag-of-words vector\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "# Print the corresponding words for each feature in the bag-of-words vector\n",
    "for i, word in enumerate(feature_names):\n",
    "    print(f\"Feature {i + 1}: '{word}' - Value: {bag_of_words[0, i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38543b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beats': 0, 'best': 0, 'both': 0, 'brazil': 2, 'germany': 0, 'is': 0, 'love': 1, 'sweden': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the feature names (words) corresponding to each feature in the bag-of-words vector\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary with words and their corresponding counts\n",
    "word_count_dict = {word: bag_of_words[0, i] for i, word in enumerate(feature_names)}\n",
    "\n",
    "# Print the word count dictionary\n",
    "print(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4898f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2989f44c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_array\n\u001b[0;32m     13\u001b[0m result_without_stop \u001b[38;5;241m=\u001b[39m stop_word_remover(text_data)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def stop_word_remover(text_array):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    result_array = np.empty_like(text_array, dtype=object)\n",
    "\n",
    "    for i, sentence in enumerate(text_array):\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_without_stop = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "        result_array[i] = sentence_without_stop\n",
    "\n",
    "    return result_array\n",
    "\n",
    "\n",
    "result_without_stop = stop_word_remover(text_data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words=count.fit_transform(result_without_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_word_count_dict(text_array):\n",
    "    # Ensure each element is a string\n",
    "    text_array = np.vectorize(str)(text_array)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in text_array]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flat_words = [word.lower() for sublist in words for word in sublist]\n",
    "\n",
    "    # Create a Counter to count the occurrences of each word\n",
    "    word_counts = Counter(flat_words)\n",
    "\n",
    "    # Convert Counter to a dictionary\n",
    "    word_count_dict = dict(word_counts)\n",
    "\n",
    "    return word_count_dict\n",
    "\n",
    "word_count_dict = create_word_count_dict(text_data)\n",
    "print(word_count_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_lists(text_array):\n",
    "    # Ensure each element is a string\n",
    "    text_array = np.vectorize(str)(text_array)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in text_array]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flat_words = [word.lower() for sublist in words for word in sublist]\n",
    "\n",
    "    # Create a Counter to count the occurrences of each word\n",
    "    word_counts = Counter(flat_words)\n",
    "\n",
    "    # Extract lists of unique words, counts, and words in order\n",
    "    unique_words = list(word_counts.keys())\n",
    "    counts = list(word_counts.values())\n",
    "    words_in_order = flat_words\n",
    "\n",
    "    return unique_words, counts, words_in_order\n",
    "\n",
    "# Example usage\n",
    "text_data = np.array([\n",
    "    'I love Brazil . Brazil!',\n",
    "    'Sweden is best',\n",
    "    'Germany beats both'\n",
    "])\n",
    "\n",
    "unique_words, counts, words_in_order = create_word_count_lists(text_data)\n",
    "\n",
    "print(\"Unique Words:\", unique_words)\n",
    "print(\"Counts:\", counts)\n",
    "print(\"Words in Order:\", words_in_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f8bff",
   "metadata": {},
   "source": [
    "## Tf idf example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([\n",
    "    'I love Brazil . Brazil!',\n",
    "    'Sweden is best',\n",
    "    'Germany beats both'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af344bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aceba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44282ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa33b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# Convert the feature matrix to an array for easier display\n",
    "feature_array = feature_matrix.toarray()\n",
    "\n",
    "# Get the feature names (words) corresponding to each column\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Print a list of words and their corresponding TF-IDF values\n",
    "word_tfidf_list = [(word, feature_array[0, i]) for i, word in enumerate(feature_names)]\n",
    "print(word_tfidf_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1271d4",
   "metadata": {},
   "source": [
    "## Punctuation removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "myString ='You,{]$%are amazing student:at@@! at @Lambton college !n;}'\n",
    "test_str = myString.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d8069",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edf40e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perceprotn_tagger: Package\n",
      "[nltk_data]     'averaged_perceprotn_tagger' not found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Codespeedy', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('programming', 'NN'), ('blog.', 'NN'), ('Blog', 'NNP'), ('posts', 'VBZ'), ('contain', 'VBP'), ('articles', 'NNS'), ('and', 'CC'), ('tutorials', 'NNS'), ('on', 'IN'), ('Python', 'NNP'), ('CSS', 'NNP'), ('and', 'CC'), ('even', 'RB'), ('much', 'RB'), ('more', 'JJR')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceprotn_tagger')\n",
    "text= ('Codespeedy is a programming blog. \" \" Blog posts contain articles and tutorials on Python, CSS and even much more')\n",
    "tb= TextBlob(text)\n",
    "print(tb.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0a2d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"Can you please buy me an Arizon Ice Tea? It is $0.99\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6345084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts of speech:  [('Can', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('buy', 'VB'), ('me', 'PRP'), ('an', 'DT'), ('Arizon', 'NNP'), ('Ice', 'NNP'), ('Tea', 'NNP'), ('?', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('$', '$'), ('0.99', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "print(\"parts of speech: \",nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d2a57",
   "metadata": {},
   "source": [
    "## practice question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class activity  tfidf bag of words etc questions like this will appear in the exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
