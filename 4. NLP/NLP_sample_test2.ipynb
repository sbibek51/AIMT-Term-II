{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f30a275",
   "metadata": {
    "id": "9f30a275"
   },
   "source": [
    "## NLP Assignment\n",
    "\n",
    "### Context\n",
    "In this assignment, you will apply various Natural Language Processing techniques to analyze and process a given text using Python. You will utilize libraries such as NLTK and scikit-learn to perform these tasks.\n",
    "\n",
    "### Sample Text\n",
    "\"In the 21st-century tech-landscape: AI (#Artificial_Intelligence), VR (Virtual Reality), and IoT (Internet of Things) are buzzwords. Yet, amidst this progress, issues such as e-waste, cyber-security, & ethical AI spark debates. It's a conundrum of 'innovate or perish', where every '@'mention' and 'like' on social media platforms can have unforeseen consequences.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e1d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd3ccc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text =\"In the 21st-century tech-landscape: AI (#Artificial_Intelligence), VR (Virtual Reality), and IoT (Internet of Things) are buzzwords. Yet, amidst this progress, issues such as e-waste, cyber-security, & ethical AI spark debates. It's a conundrum of 'innovate or perish', where every '@'mention' and 'like' on social media platforms can have unforeseen consequences.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e21a80",
   "metadata": {
    "id": "86e21a80"
   },
   "source": [
    "**Task 1: Text Cleaning**\n",
    "- Remove all punctuation from the text.\n",
    "- Convert all text to lowercase.\n",
    "- Remove all stopwords (use NLTK's predefined list of stopwords).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6c8123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 21st-century tech-landscape: AI (#Artificial_Intelligence), VR (Virtual Reality), and IoT (Internet of Things) are buzzwords. Yet, amidst this progress, issues such as e-waste, cyber-security, & ethical AI spark debates. It's a conundrum of 'innovate or perish', where every '@'mention' and 'like' on social media platforms can have unforeseen consequences.\n"
     ]
    }
   ],
   "source": [
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a6fbc",
   "metadata": {},
   "source": [
    "## Step 1: converting to lower text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f983679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text =my_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "230a82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the 21st-century tech-landscape: ai (#artificial_intelligence), vr (virtual reality), and iot (internet of things) are buzzwords. yet, amidst this progress, issues such as e-waste, cyber-security, & ethical ai spark debates. it's a conundrum of 'innovate or perish', where every '@'mention' and 'like' on social media platforms can have unforeseen consequences.\n"
     ]
    }
   ],
   "source": [
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f48ef",
   "metadata": {},
   "source": [
    "## Step 2: Removing the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c6612ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the punctuation\n",
    "my_text = my_text.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e6fd12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the 21stcentury techlandscape ai artificialintelligence vr virtual reality and iot internet of things are buzzwords yet amidst this progress issues such as ewaste cybersecurity  ethical ai spark debates its a conundrum of innovate or perish where every mention and like on social media platforms can have unforeseen consequences'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b71bb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aea0010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb22779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce90c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text_tokenized = word_tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e96da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', '21stcentury', 'techlandscape', 'ai', 'artificialintelligence', 'vr', 'virtual', 'reality', 'and', 'iot', 'internet', 'of', 'things', 'are', 'buzzwords', 'yet', 'amidst', 'this', 'progress', 'issues', 'such', 'as', 'ewaste', 'cybersecurity', 'ethical', 'ai', 'spark', 'debates', 'its', 'a', 'conundrum', 'of', 'innovate', 'or', 'perish', 'where', 'every', 'mention', 'and', 'like', 'on', 'social', 'media', 'platforms', 'can', 'have', 'unforeseen', 'consequences']\n"
     ]
    }
   ],
   "source": [
    "print(my_text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e3b62",
   "metadata": {},
   "source": [
    "### 3.Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cbe9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21stcentury', 'techlandscape', 'ai', 'artificialintelligence', 'vr', 'virtual', 'reality', 'iot', 'internet', 'things', 'buzzwords', 'yet', 'amidst', 'progress', 'issues', 'ewaste', 'cybersecurity', 'ethical', 'ai', 'spark', 'debates', 'conundrum', 'innovate', 'perish', 'every', 'mention', 'like', 'social', 'media', 'platforms', 'unforeseen', 'consequences']\n"
     ]
    }
   ],
   "source": [
    "my_text_filtered_stop_words= []\n",
    "for token in my_text_tokenized:\n",
    "    if token not in stopwords:\n",
    "        my_text_filtered_stop_words.append(token)\n",
    "print(my_text_filtered_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5eb6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21stcentury', 'techlandscape', 'ai', 'artificialintelligence', 'vr', 'virtual', 'reality', 'iot', 'internet', 'things', 'buzzwords', 'yet', 'amidst', 'progress', 'issues', 'ewaste', 'cybersecurity', 'ethical', 'ai', 'spark', 'debates', 'conundrum', 'innovate', 'perish', 'every', 'mention', 'like', 'social', 'media', 'platforms', 'unforeseen', 'consequences']\n"
     ]
    }
   ],
   "source": [
    "print(my_text_filtered_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1cc8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words_string = ' '.join(my_text_filtered_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e3260c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21stcentury techlandscape ai artificialintelligence vr virtual reality iot internet things buzzwords yet amidst progress issues ewaste cybersecurity ethical ai spark debates conundrum innovate perish every mention like social media platforms unforeseen consequences\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343021ed",
   "metadata": {
    "id": "343021ed"
   },
   "source": [
    "**Task 2: Custom Tokenization**\n",
    "- Use NLTK's `RegexpTokenizer` to create a custom tokenizer that only captures words of three or more letters.\n",
    "- Compare the results of your custom tokenization with the default tokenization behavior of `CountVectorizer` or `TfidfVectorizer` from `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35de472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e09a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens after custom tokenization:\n",
      " ['21stcentury', 'techlandscape', 'artificialintelligence', 'virtual', 'reality', 'iot', 'internet', 'things', 'buzzwords', 'yet', 'amidst', 'progress', 'issues', 'ewaste', 'cybersecurity', 'ethical', 'spark', 'debates', 'conundrum', 'innovate', 'perish', 'every', 'mention', 'like', 'social', 'media', 'platforms', 'unforeseen', 'consequences']\n"
     ]
    }
   ],
   "source": [
    "# defining a pattern to capture three or more words\n",
    "pattern = r'\\b\\w{3,}\\b'\n",
    "\n",
    "custom_tokenizer =RegexpTokenizer(pattern)\n",
    "# Tokenize the preprocessed text using the custom tokenizer\n",
    "tokens_custom= custom_tokenizer.tokenize(filtered_words_string)\n",
    "\n",
    "print('Tokens after custom tokenization:\\n',tokens_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be713a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "308af304",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f02e6a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21stcentury' 'ai' 'amidst' 'artificialintelligence' 'buzzwords'\n",
      " 'consequences' 'conundrum' 'cybersecurity' 'debates' 'ethical' 'every'\n",
      " 'ewaste' 'innovate' 'internet' 'iot' 'issues' 'like' 'media' 'mention'\n",
      " 'perish' 'platforms' 'progress' 'reality' 'social' 'spark'\n",
      " 'techlandscape' 'things' 'unforeseen' 'virtual' 'vr' 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the preprocessed text\n",
    "X = vectorizer.fit_transform(my_text_filtered_stop_words) # this methods expect to take list not string\n",
    "\n",
    "# Get the feature names (tokens)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce3616a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of tokens generated from custom tokenization is: 29\n",
      "numbers of tokens generated from countvectorizer tokenization is: 31\n",
      "tokens that are generated by countvectorizer but not by custom tokenizer are:\n",
      "ai\n",
      "vr\n"
     ]
    }
   ],
   "source": [
    "# comparing the result of custom tokenization with countvectorizer\n",
    "print('numbers of tokens generated from custom tokenization is:',len(tokens_custom))\n",
    "print('numbers of tokens generated from countvectorizer tokenization is:',len(feature_names))\n",
    "\n",
    "\n",
    "print('tokens that are generated by countvectorizer but not by custom tokenizer are:')\n",
    "for token in feature_names:\n",
    "    if token not in tokens_custom:\n",
    "        print(token)\n",
    "\n",
    "        \n",
    "# for token in tokens_custom:\n",
    "#     if token not in feature_names:\n",
    "#         print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9476b6",
   "metadata": {
    "id": "3f9476b6"
   },
   "source": [
    "**Task 3: Bag of Words Model**\n",
    "- Construct a Bag of Words model using your cleaned and tokenized text from Task 1 and Task 2, and display the word frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ca4a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (1, 24)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 27)\t1\n",
      "  (4, 21)\t1\n",
      "  (5, 13)\t1\n",
      "  (6, 12)\t1\n",
      "  (7, 25)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 28)\t1\n",
      "  (10, 1)\t1\n",
      "  (11, 20)\t1\n",
      "  (12, 14)\t1\n",
      "  (13, 10)\t1\n",
      "  (14, 6)\t1\n",
      "  (15, 8)\t1\n",
      "  (16, 23)\t1\n",
      "  (17, 7)\t1\n",
      "  (18, 5)\t1\n",
      "  (19, 11)\t1\n",
      "  (20, 18)\t1\n",
      "  (21, 9)\t1\n",
      "  (22, 17)\t1\n",
      "  (23, 15)\t1\n",
      "  (24, 22)\t1\n",
      "  (25, 16)\t1\n",
      "  (26, 19)\t1\n",
      "  (27, 26)\t1\n",
      "  (28, 4)\t1\n",
      "\n",
      "\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "21stcentury: 1\n",
      "amidst: 1\n",
      "artificialintelligence: 1\n",
      "buzzwords: 1\n",
      "consequences: 1\n",
      "conundrum: 1\n",
      "cybersecurity: 1\n",
      "debates: 1\n",
      "ethical: 1\n",
      "every: 1\n",
      "ewaste: 1\n",
      "innovate: 1\n",
      "internet: 1\n",
      "iot: 1\n",
      "issues: 1\n",
      "like: 1\n",
      "media: 1\n",
      "mention: 1\n",
      "perish: 1\n",
      "platforms: 1\n",
      "progress: 1\n",
      "reality: 1\n",
      "social: 1\n",
      "spark: 1\n",
      "techlandscape: 1\n",
      "things: 1\n",
      "unforeseen: 1\n",
      "virtual: 1\n",
      "yet: 1\n"
     ]
    }
   ],
   "source": [
    "# constructing bow  and displaying the word frequencies:\n",
    "X = vectorizer.fit_transform(tokens_custom)\n",
    "\n",
    "print(X)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(X.toarray())\n",
    "\n",
    "# Get the feature names (words)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the word frequencies\n",
    "word_frequencies = X.toarray().sum(axis=0)\n",
    "\n",
    "# Display word frequencies\n",
    "for word, freq in zip(words, word_frequencies):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c69b29",
   "metadata": {
    "id": "f5c69b29"
   },
   "source": [
    "**Task 4: Analyze Word Importance with TF-IDF**\n",
    "- Apply `TfidfVectorizer` to the sample text to compute the TF-IDF scores for each word in the document.\n",
    "- Comment on the differences between TF-IDF scores and word frequencies from the Bag of Words model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3906abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1770383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "21stcentury: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "ai: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "amidst: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "artificialintelligence: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "buzzwords: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "consequences: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "conundrum: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "cybersecurity: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "debates: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "ethical: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "every: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "ewaste: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "innovate: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "internet: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "iot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "issues: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "like: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "media: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "mention: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "perish: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "platforms: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "progress: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "reality: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "social: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "spark: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "techlandscape: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "things: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "unforeseen: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "virtual: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "vr: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "yet: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the filtered text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(feature_names)\n",
    "\n",
    "# Get the feature names (words)\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the TF-IDF scores\n",
    "tfidf_scores = tfidf_matrix.toarray()\n",
    "\n",
    "# Display TF-IDF scores for each word\n",
    "for i, word in enumerate(words):\n",
    "    print('\\n')\n",
    "    print(f\"{word}: {tfidf_scores[:, i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524487dc",
   "metadata": {
    "id": "524487dc"
   },
   "source": [
    "**Task 5: Part-of-Speech Tagging**\n",
    "- Perform POS tagging on the cleaned and tokenized text.\n",
    "- Identify the most common part of speech in the sample text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0abcb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21stcentury techlandscape ai artificialintelligence vr virtual reality iot internet things buzzwords yet amidst progress issues ewaste cybersecurity ethical ai spark debates conundrum innovate perish every mention like social media platforms unforeseen consequences\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "tb= TextBlob(filtered_words_string)\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "660f4590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common part of speech: ('NN', 10)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_words = tb.tags\n",
    "\n",
    "# Extract POS tags\n",
    "pos_tags = [tag for word, tag in tagged_words]\n",
    "\n",
    "# Count the occurrences of each POS tag\n",
    "pos_tag_counts = Counter(pos_tags)\n",
    "\n",
    "# Find the most common POS tag\n",
    "most_common_pos_tag = pos_tag_counts.most_common(1)[0]\n",
    "\n",
    "# Display the most common POS tag\n",
    "print(\"Most common part of speech:\", most_common_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f4e32",
   "metadata": {},
   "source": [
    "The part of speech 'NN' indicates a noun, which is the most common part of speech in our text.\n",
    "The number '10' represents the frequency of occurrences of this part of speech in our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a464c",
   "metadata": {
    "id": "110a464c"
   },
   "source": [
    "**Task 6: Working with Synonyms and Antonyms**\n",
    "- Choose three words from the text and use WordNet to find their synonyms and antonyms.\n",
    "- Discuss the role of synonyms and antonyms in text analysis and how they could affect the interpretation of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee3c1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_words =['virtual', 'reality','progress']\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3323c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: virtual\n",
      "Synonyms: {'virtual', 'practical'}\n",
      "Antonyms: set()\n",
      "Word: reality\n",
      "Synonyms: {'realism', 'world', 'reality', 'realness'}\n",
      "Antonyms: {'unreality'}\n",
      "Word: progress\n",
      "Synonyms: {'pass_on', 'go_on', 'get_along', 'build', 'shape_up', 'advance', 'forward_motion', 'get_on', 'onward_motion', 'work_up', 'come_on', 'come_along', 'progression', 'march_on', 'move_on', 'advancement', 'progress', 'procession', 'build_up'}\n",
      "Antonyms: {'recede', 'retreat', 'regress'}\n"
     ]
    }
   ],
   "source": [
    "# Function to find synonyms and antonyms\n",
    "def find_synonyms_antonyms(word):\n",
    "    # Synonyms and antonyms lists\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    # Iterate over each synset of the word in WordNet\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            # Add synonyms\n",
    "            synonyms.append(lemma.name())\n",
    "\n",
    "            # Add antonyms\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "    # Return the sets of synonyms and antonyms\n",
    "    return set(synonyms), set(antonyms)\n",
    "\n",
    "# Word to find synonyms and antonyms for\n",
    "word = 'virtual'\n",
    "\n",
    "# Call the function\n",
    "synonyms, antonyms = find_synonyms_antonyms(word)\n",
    "\n",
    "# Print the synonyms and antonyms\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Synonyms: {synonyms}\")\n",
    "print(f\"Antonyms: {antonyms}\")\n",
    "\n",
    "# Word to find synonyms and antonyms for\n",
    "word = 'reality'\n",
    "\n",
    "# Call the function\n",
    "synonyms, antonyms = find_synonyms_antonyms(word)\n",
    "\n",
    "# Print the synonyms and antonyms\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Synonyms: {synonyms}\")\n",
    "print(f\"Antonyms: {antonyms}\")\n",
    "\n",
    "# Word to find synonyms and antonyms for\n",
    "word = 'progress'\n",
    "\n",
    "# Call the function\n",
    "synonyms, antonyms = find_synonyms_antonyms(word)\n",
    "\n",
    "# Print the synonyms and antonyms\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Synonyms: {synonyms}\")\n",
    "print(f\"Antonyms: {antonyms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec86267",
   "metadata": {
    "id": "1ec86267"
   },
   "source": [
    "**Task 7: Text Normalization**\n",
    "- Apply stemming and lemmatization to the cleaned and tokenized text.\n",
    "- Compare the results and discuss the circumstances where each method would be preferred over the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91870332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['21stcenturi', 'techlandscap', 'ai', 'artificialintellig', 'vr', 'virtual', 'realiti', 'iot', 'internet', 'thing', 'buzzword', 'yet', 'amidst', 'progress', 'issu', 'ewast', 'cybersecur', 'ethic', 'ai', 'spark', 'debat', 'conundrum', 'innov', 'perish', 'everi', 'mention', 'like', 'social', 'media', 'platform', 'unforeseen', 'consequ']\n",
      "\n",
      "\n",
      "Lemmatized words: ['21stcentury', 'techlandscape', 'ai', 'artificialintelligence', 'vr', 'virtual', 'reality', 'iot', 'internet', 'thing', 'buzzword', 'yet', 'amidst', 'progress', 'issue', 'ewaste', 'cybersecurity', 'ethical', 'ai', 'spark', 'debate', 'conundrum', 'innovate', 'perish', 'every', 'mention', 'like', 'social', 'medium', 'platform', 'unforeseen', 'consequence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "tokens = word_tokenize(filtered_words_string)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize WordNetLemmatizer for lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform stemming and lemmatization\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Print stemmed and lemmatized words\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print('\\n')\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84833b2e",
   "metadata": {},
   "source": [
    "Lemmatization and stemming are both techniques used to reduce words to their base forms, but they differ in their approaches and outcomes. Lemmatization preserves the semantic meaning of words by reducing them to their dictionary form, accounting for variations in word morphology and part of speech. In contrast, stemming applies simpler rules to truncate words to their root form, often resulting in less accurate reductions and potential loss of semantic meaning. While lemmatization offers higher precision and linguistic accuracy, stemming is preferred in scenarios where computational efficiency and simplicity are prioritized over linguistic precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d9bd4",
   "metadata": {
    "id": "4d0d9bd4"
   },
   "source": [
    "**Task 8: Spell Correction**\n",
    "- Implement a spell correction algorithm or use a library to correct the misspelled words in the sample text.\n",
    "- Discuss how spell correction can impact the sentiment or meaning of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e7e41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21stcentury techlandscape ai artificialintelligence vr virtual reality iot internet things buzzwords yet amidst progress issues ewaste cybersecurity ethical ai spark debates conundrum innovate perish every mention like social media platforms unforeseen consequences\n",
      "\n",
      "\n",
      "Corrected text: 21century techlandscape ai artificialintelligence vr virtual reality iot internet things buzzwords yet amidst progress issues waste cybersecurity ethical ai spark debates conundrum innovate perish every mention like social media platforms unforeseen consequences\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "speller = Speller()\n",
    "\n",
    "# Correct misspelled words\n",
    "corrected_text = speller(filtered_words_string)\n",
    "\n",
    "# Print the corrected text\n",
    "print(filtered_words_string)\n",
    "print('\\n')\n",
    "print(\"Corrected text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6166e0",
   "metadata": {
    "id": "eb6166e0"
   },
   "source": [
    "**Task 9: Similarity Measurement**\n",
    "- Using WordNet synsets, calculate the semantic similarity between the different uses of the word \"bank\" in the context of a financial institution and the side of a river.\n",
    "- Explain how semantic similarity can be useful in disambiguating word meanings in NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c591aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity score: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# Define the two senses of the word \"bank\"\n",
    "financial_bank = wordnet.synsets('bank', pos='n')[0]  # Synset for financial institution\n",
    "river_bank = wordnet.synsets('bank', pos='n')[1]      # Synset for side of a river\n",
    "\n",
    "# Calculate the Wu-Palmer Similarity\n",
    "similarity_score = financial_bank.wup_similarity(river_bank)\n",
    "\n",
    "# Print the semantic similarity score\n",
    "print(\"Semantic similarity score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86683404",
   "metadata": {
    "id": "86683404"
   },
   "source": [
    "**Task 10: Fixing Word Lengthening**\n",
    "- Write a function to identify words with characters repeated more than twice in a row and shorten the repetition to two characters.\n",
    "- Discuss the impact of correcting lengthened words on text analysis and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "563d9a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: woooow\n",
      "Shortened word: woow\n",
      "\n",
      "Original word: goooal\n",
      "Shortened word: gooal\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def shorten_repeated_chars(word):\n",
    "    # Use regular expression to find repeated characters more than twice\n",
    "    shortened_word = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    return shortened_word\n",
    "\n",
    "# Test the function with the word \"woooow\" and \"goooal\"\n",
    "word1 = \"woooow\"\n",
    "word2 = \"goooal\"\n",
    "\n",
    "# Shorten the repeated characters\n",
    "shortened_word1 = shorten_repeated_chars(word1)\n",
    "shortened_word2 = shorten_repeated_chars(word2)\n",
    "\n",
    "# Print the shortened words\n",
    "print(\"Original word:\", word1)\n",
    "print(\"Shortened word:\", shortened_word1)\n",
    "\n",
    "print(\"\\nOriginal word:\", word2)\n",
    "print(\"Shortened word:\", shortened_word2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e7946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "title": "NLP Assignment"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
